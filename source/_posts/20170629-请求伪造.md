---
title: 请求伪造
date: 2017-06-29 09:04:52
tags: 爬虫
---
&emsp;&emsp;爬虫蜘蛛请求web网页时，一般Nginx,Apache等web服务器的日志会有请求信息User-Agent、Referer,Ip等信息。为了防止禁掉爬虫，我们可以通过伪造User-Agent、Referer、ip等来实现反爬功能。（下文作者将通过实例来说明）



_ _ _



```python

#coding:utf-8

import requests

import sys

reload(sys)

sys.setdefaultencoding("utf-8")

response = requests.get(url="http://www.xiujinniu.com")

```

当运行时，web日志会显示如下：（爬虫的IP和python脚本执行的请求）

```nginx

124.16.136.100 - - [01/Apr/2017:13:02:21 +0800] "GET / HTTP/1.1" 200 381 "-" "python-requests/2.12.4"

```



##### 伪造 User-Agent

User Agent中文名为用户代理，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU 类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等。一些网站常常通过判断 UA 来给不同的操作系统、不同的浏览器发送不同的页面，因此可能造成某些页面无法在某个浏览器中正常显示，但通过伪装 UA 可以绕过检测。

```python

headers = {

    "User-Agent":"Mozilla/4.0 (compatible; MSIE 6.0; AOL 9.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2)"

}

response = requests.get(url="http://www.xiujinniu.com",headers=headers)

```

web日志显示

```nginx

124.16.136.100 - - [01/Apr/2017:13:10:26 +0800] "GET / HTTP/1.1" 200 381 "-" "Mozilla/4.0 (compatible; MSIE 6.0; AOL 9.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2)"

```

##### 伪造 Referer

HTTP Referer是header的一部分，当浏览器向web服务器发送请求的时候，一般会带上Referer，告诉服务器我是从哪个页面链接过来的，服务器藉此可以获得一些信息用于处理。比如从我主页上链接到一个朋友那里，他的服务器就能够从HTTP Referer中统计出每天有多少用户点击我主页上的链接访问他的网站。

```python

headers = {

    "Referer":"https://www.baidu.com"

}

response = requests.get(url="http://www.xiujinniu.com",headers=headers)

```

web日志显示

```nginx

124.16.136.100 - - [01/Apr/2017:13:14:35 +0800] "GET / HTTP/1.1" 200 381 "https://www.baidu.com" "python-requests/2.12.4"

```

##### 伪造 IP

当爬虫蜘蛛请求web时，web服务器会显示发送请求的原始IP，我们可以将挂代理来防止本地IP被禁。

```python

proxies = {

    "http": "http://124.88.67.32:81",

    "https": "http://124.88.67.32:81"

}

response = requests.get(url="http://www.xiujinniu.com",proxies=proxies)

```

web日志显示

```nginx

124.88.67.32 - - [01/Apr/2017:13:16:25 +0800] "GET http://www.xiujinniu.com/ HTTP/1.1" 200 381 "-" "python-requests/2.12.4"

```

### 总结

为了防止爬虫被禁可以将ip,referer,user-agent同时写入爬虫，这样web就无法判断该请求是否是爬虫发起。
